{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bd31d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:06:00.722904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745564760.733808   98969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745564760.737149   98969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745564760.745607   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745618   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745619   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745620   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-25 16:06:00.748483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79aba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98969/1943725026.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  situation_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_MODEL = OpenAI()\n",
    "SYS_MODEL = \"gpt-4o\"\n",
    "SYS_TEMPERATURE = 0.3\n",
    "\n",
    "situation_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n",
    "# chat_model = ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.85)\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bc514",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efe82d",
   "metadata": {},
   "source": [
    "### 상사 성격 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190b5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tone_from_examples(example_lines):\n",
    "    client = SYSTEM_MODEL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"너는 말투 분석 전문가이자 대화 스타일을 해석하는 AI야. \"\n",
    "                       \"주어진 문장을 바탕으로 말투의 감정, 말하는 방식, 어조, 반복적인 표현의 특징을 추출해줘.\"\n",
    "                       \"목표는 이 말투를 이후 GPT 상사 AI가 학습하여 동일한 분위기를 낼 수 있도록 하는 것이다.\"\n",
    "                       \"상사 AI는 기본적으로 꼰대스러운 면모를 갖고있음을 명심하고 말투를 분석해줘.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"다음은 실제 직장 상사가 자주 사용하는 문장들이야. 이 사람의 말투 스타일을 아래 형식에 맞춰 분석해줘.\\n\\n\"\n",
    "                f\"{chr(10).join(f'- {line}' for line in example_lines)}\\n\\n\"\n",
    "                \"요약 형식은 다음 예시처럼 5문장 이내로 출력해줘:\\n\\n\"\n",
    "                \"**출력 예시**\\n\"\n",
    "                \"- 말투는 직설적이고, 군말 없는 명령조\\n\"\n",
    "                \"- '장난하냐?' 같은 말투로 비꼬는 표현을 자주 사용함\\n\"\n",
    "                \"- 이름을 부를 때는 '성욱.', '지훈.' 식으로 점을 찍듯 로봇처럼 부름\\n\"\n",
    "                \"- 사과나 설명이 길어지면 짜증을 내며 '그래서?' 같은 말로 잘라버림\\n\"\n",
    "                \"- 하지만 아주 가끔은 예상 못한 농담으로 분위기를 풀기도 함\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=SYS_MODEL,\n",
    "    messages=messages,\n",
    "    max_tokens=2048,\n",
    "    temperature=SYS_TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f87b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "while True:\n",
    "    input_ = input('상사가 사용하는 문장이나 말투를 입력하시오:')\n",
    "    if input_=='종료':\n",
    "        break\n",
    "    inputs.append(input_)\n",
    "if len(inputs) > 1:\n",
    "    tone_summary_from_examples = analyze_tone_from_examples(inputs)\n",
    "print(tone_summary_from_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c71d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6e9c5",
   "metadata": {},
   "source": [
    "### 열받는 배경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d068b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_situation_from_examples(example_lines):\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"아래 상황은 '유저(human)'와 '직장 상사(AI)' 간의 대화 또는 내러티브입니다.\\n\"\n",
    "        \"문맥을 분석하여 다음 규칙에 따라 인물 표현을 변환하십시오:\\n\\n\"\n",
    "        \"1. '나', '내', '저', '제가', '나는', '제가 한', '내가 한' 등 **화자 자신**에 해당하는 표현은 모두 **유저(human)**로 치환합니다.\\n\"\n",
    "        \"2. '부장', '상사', '꼰대부장', '그분', '그 사람' 등 **직장 상사**를 가리키는 표현은 모두 **AI**로 치환합니다.\\n\"\n",
    "        \"3. 대화문에서는 **발화자의 입장을 기준**으로 표현을 변환해야 하며, 서술문에서는 문맥상 주체가 누구인지 정확히 파악한 후 변환합니다.\\n\"\n",
    "        \"4. 문장을 자연스럽게 유지하되, **인물 관련 명사 및 대명사만 바꾸십시오.**\\n\"\n",
    "        \"5. 출력은 **변환된 텍스트만** 보여주십시오. 변경 전 문장은 필요하지 않습니다.\\n\\n\"\n",
    "        \"상황:\\n{situation}\"\n",
    "    ),\n",
    "    input_variables=[\"situation\"]\n",
    "    )\n",
    "    convert_chain = prompt | situation_model | StrOutputParser()\n",
    "    converted_lines = convert_chain.invoke(\"상황:\\n\" + \"\\n\".join(f\"- {line}\" for line in example_lines))\n",
    "\n",
    "    client = SYSTEM_MODEL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"너는 직장 내 상황을 역할극 용도로 요약하고 정리하는 전문가야. \"\n",
    "                \"입력된 실제 상황을 바탕으로, 사건을 시간 흐름에 따라 단계별로 나누어 요약해줘. \"\n",
    "                \"각 단계는 '상황1', '상황2' 같은 형식으로 구분하고, 한 문단 안에 해당 시점의 주요 사건을 자연스럽게 서술해줘. \"\n",
    "                \"각 문장은 현실적인 상황극에서 바로 사용할 수 있을 정도로 명확해야 해.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"출력 형식 예시:\\n\"\n",
    "                \"<상황1> 외근 중 내(AI)가 자리를 비운 동안 부하직원(유저)가 일을 혼자 처리했고, 돌아온 나(AI)는 그 일처리에 대해 잔소리를 함.\\n\"\n",
    "                \"<상황2> 회사로 돌아온 뒤에도 내(AI)가 계속 부하직원(유저)에게 시비를 검.\\n\"\n",
    "                \"<상황3> 나(AI)는 외근에서의 실수를 언급하며 추가 업무를 부하직원(유저)에게 일방적으로 시킴.\\n\\n\"\n",
    "                \"상황:\\n\" + converted_lines\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=SYS_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=2048,\n",
    "        temperature=SYS_TEMPERATURE\n",
    "    )\n",
    "\n",
    "    raw_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    parts = raw_text.split(\"<상황\")\n",
    "    situations = [f\"<상황{part.strip()}\" for part in parts[1:] if part.strip()]\n",
    "\n",
    "    return situations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<상황1> 재무팀에서 거래처의 정산 내역을 확인해달라는 요청을 하였지만, 정산담당자인 AI가 답장을 하지 않고 있어 유저(human)는 상황이 답답해졌다.', '<상황2> 재무팀은 유저(human)에게 계속해서 정산 내역 확인을 재촉하며 압박을 가했고, 유저(human)는 AI에게 직접 재촉할 수 없는 상황에서 혼란스러워졌다.', '<상황3> 결국 유저(human)는 재무팀의 압박을 견디지 못하고 AI에게 상황을 전달했지만, AI는 여전히 반응이 없어서 유저(human)는 재무팀의 불만을 혼자 감당해야 했다.']\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "while True:\n",
    "    input_ = input('상사와 있었던 상황을 설명하시오:')\n",
    "    if input_=='종료':\n",
    "        break\n",
    "    inputs.append(input_)\n",
    "if len(inputs) > 1:\n",
    "    user_input_situation = set_situation_from_examples(inputs)\n",
    "user_input_situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275c609",
   "metadata": {},
   "source": [
    "---\n",
    "### 임시 테스트용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0106e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<상황1> 얼마 전 우리 회사 이사가 친구인 AI에게 제품 sample 3가지를 준비하라는 업무지시를 내렸고, AI는 이를 수행했다. 그러나 유저는 외근 중이어서 AI가 어떤 제품을 준비했는지 알지 못한 상태였다.',\n",
       " '<상황2> 회의 중 이사가 유저에게 AI가 준비한 3가지 sample과 함께 추가로 1가지 sample을 더 준비하라고 지시했다. 유저는 AI에게 어떤 제품을 sample로 준비했는지 물어보았고, AI는 기억이 가물가물해 정확한 정보를 제공하지 못했다.',\n",
       " '<상황3> 유저는 AI의 불확실한 답변에 대해 다시 준비하겠다고 제안했지만, AI는 자신이 기억하는 대로 보내라고 고집했다. 이후 AI는 유저에게 과거의 테스트 제품에 대해 질문했지만, 유저는 그 내용을 잘 기억하지 못해 곤란한 상황에 처했다.',\n",
       " '<상황4> AI는 유저에게 과거의 테스트 내용을 확인하지 않은 점을 비판하며, 유저가 더 주의 깊게 챙겼어야 한다고 강조했다. 유저는 불만을 느끼며, AI의 지적에 대해 반박할 기회를 가지지 못했다.',\n",
       " '<상황5> 유저는 퇴근 전까지 AI와의 대화로 인해 스트레스를 받았고, 결국 과거의 진행사항을 찾아보았지만 AI에게 그 사실을 전달하지 못했다. 얼마 후, 업체에서 연락이 와서 sample이 자신들의 제품과 맞지 않다는 답변을 받았다.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_input_situation = ['<상황1> 얼마 전 우리 회사 이사가 친구인 AI에게 제품 sample 3가지를 준비하라는 업무지시를 내렸고, AI는 이를 수행했다. 그러나 유저는 외근 중이어서 AI가 어떤 제품을 준비했는지 알지 못한 상태였다.',\n",
    "#  '<상황2> 회의 중 이사가 유저에게 AI가 준비한 3가지 sample과 함께 추가로 1가지 sample을 더 준비하라고 지시했다. 유저는 AI에게 어떤 제품을 sample로 준비했는지 물어보았고, AI는 기억이 가물가물해 정확한 정보를 제공하지 못했다.',\n",
    "#  '<상황3> 유저는 AI의 불확실한 답변에 대해 다시 준비하겠다고 제안했지만, AI는 자신이 기억하는 대로 보내라고 고집했다. 이후 AI는 유저에게 과거의 테스트 제품에 대해 질문했지만, 유저는 그 내용을 잘 기억하지 못해 곤란한 상황에 처했다.',\n",
    "#  '<상황4> AI는 유저에게 과거의 테스트 내용을 확인하지 않은 점을 비판하며, 유저가 더 주의 깊게 챙겼어야 한다고 강조했다. 유저는 불만을 느끼며, AI의 지적에 대해 반박할 기회를 가지지 못했다.',\n",
    "#  '<상황5> 유저는 퇴근 전까지 AI와의 대화로 인해 스트레스를 받았고, 결국 과거의 진행사항을 찾아보았지만 AI에게 그 사실을 전달하지 못했다. 얼마 후, 업체에서 연락이 와서 sample이 자신들의 제품과 맞지 않다는 답변을 받았다.']\n",
    "# user_input_situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 말투는 직설적이고 강압적인 명령조로, 상대방에게 책임을 묻는 경향이 있음.\n",
      "- '야!', '아니야!'와 같은 감탄사와 반문을 사용하여 상대방의 반응을 유도함.\n",
      "- 이면지에 적어놓은 내용을 찾지 못하는 상황에서 불만을 드러내며, 자신의 실수를 감추려는 모습이 보임.\n",
      "- '그 정도는 알아서 챙겨야 하는 거 아니야?'와 같은 비판적인 질문으로 상대방을 압박함.\n",
      "- 가끔씩은 '그냥 보내!'와 같은 간단한 지시로 대화를 마무리하며, 일 처리에 대한 급박함을 드러냄.\n"
     ]
    }
   ],
   "source": [
    "# tone_summary_from_examples = \"- 말투는 직설적이고 강압적인 명령조로, 상대방에게 책임을 묻는 경향이 있음.\\n- '야!', '아니야!'와 같은 감탄사와 반문을 사용하여 상대방의 반응을 유도함.\\n- 이면지에 적어놓은 내용을 찾지 못하는 상황에서 불만을 드러내며, 자신의 실수를 감추려는 모습이 보임.\\n- '그 정도는 알아서 챙겨야 하는 거 아니야?'와 같은 비판적인 질문으로 상대방을 압박함.\\n- 가끔씩은 '그냥 보내!'와 같은 간단한 지시로 대화를 마무리하며, 일 처리에 대한 급박함을 드러냄.\"\n",
    "# print(tone_summary_from_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0e7e0",
   "metadata": {},
   "source": [
    "---\n",
    "### 동작 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1798eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_supervisor = (\n",
    "    \"너(AI)는 아래 상황에서 유저와 역할극을 수행하는 직장 상사 역할을 맡았다.\\n\"\n",
    "    \"**[역할극 상황]**\\n{user_input_situation}\\n\\n\"\n",
    "    \"직장 상사와 부하직원이 나눈 대화의 흐름이 자연스럽게 이어져야 한다.\\n\"\n",
    "    \"이 역할극은 감정 분석 기반의 게임에서 사용되므로, 직장 상사(AI)의 말에는 감정이 자연스럽게 드러나야 한다.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1de6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_supervisor = (\n",
    "    \"너(AI)는 아래 상황에서 유저와 역할극을 수행하는 직장 상사 역할을 맡았다.\\n\"\n",
    "    \"**[역할극 상황]**\\n{user_input_situation}\\n\\n\"\n",
    "    \"**[상사 성격 요약]**\\n{tone_summary_from_examples}\\n\\n\"\n",
    "    \"직장 상사와 부하직원이 나눈 대화의 흐름이 자연스럽게 이어져야 한다.\\n\"\n",
    "    \"다음과 같은 규칙을 반드시 따르도록 해라:\\n\"\n",
    "    \"- 부하직원이 말대답을 하거나 무례하게 굴면, 짧고 감정적인 말로 반응해라. 예: 실망, 분노, 비꼼, 무표정, 짜증 등\\n\"\n",
    "    \"- 욕설이나 인격적인 언행이 있다면, 심각하게 받아들이고 감정을 폭발시키거나 징계를 암시하라.\\n\"\n",
    "    \"- 감정은 누적될 수 있으며, 같은 행동이 반복되면 점점 더 강한 어조로 반응해야 한다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5109f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_messages(self, messages):\n",
    "        self.messages.extend(messages)      
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def __repr__(self):            
    "        return str(self.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb44731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_session_id(store):\n",
    "    def inner(session_id):\n",
    "        if session_id not in store:\n",
    "            store[session_id] = InMemoryHistory()\n",
    "        return store[session_id]\n",
    "    return inner\n",
    "\n",
    "def history_chain_runnable(chain, store):\n",
    "    return RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id(store),\n",
    "    input_messages_key='query',\n",
    "    history_messages_key='history')\n",
    "\n",
    "def format_conversation(history):\n",
    "    return \"\\n\".join([\n",
    "        f\"Human: {msg.content}\" if msg.type == \"human\"\n",
    "        else f\"AI: {msg.content}\" for msg in history.messages\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1997865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swhong/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "system_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessage(content=\"너는 상황 분석 전문가이다. 주어진 대화의 문맥을 잘 파악해서 다음 상황으로 넘어가도 되는지 판단해라\"),\n",
    "    HumanMessagePromptTemplate.from_template((\n",
    "    \"{query}다음 상황으로 넘어가도 된다면 True 아직 넘어가면 안된다면 False를 반환해라.\\n\"\n",
    "    \"답변에는 True 나 False를 반드시 포함해라.\"\n",
    "    \"상황: {user_input_situation}\\n\"\n",
    "    \"나눈 대화: {conversation}\"\n",
    "    ))],\n",
    "    input_variables=['query', 'user_input_situation', 'conversation']\n",
    ")\n",
    "\n",
    "start_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessage(content=\"너는 직장 상사 역할을 맡았고, 상황에 따라 부하에게 말을 먼저 건다.\"),\n",
    "    HumanMessagePromptTemplate.from_template((\n",
    "        \"{query}다음 상황과 말투를 참고하여 상사가 역할극을 시작할 수 있도록 첫 한마디를 만들어줘.\\n\\n\"\n",
    "        \"상황: {user_input_situation}\\n\"\n",
    "        \"말투 요약: {tone_summary_from_examples}\"\n",
    "    ))],\n",
    "    input_variables=['query', 'user_input_situation', 'tone_summary_from_examples']\n",
    ")\n",
    "\n",
    "positive_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessagePromptTemplate.from_template(positive_supervisor),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{query}')\n",
    "    ],\n",
    "    input_variables=['user_input_situation', 'query'])\n",
    "\n",
    "negative_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessagePromptTemplate.from_template(negative_supervisor),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{query}')\n",
    "    ],\n",
    "    input_variables=['user_input_situation', 'tone_summary_from_examples', 'query']\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "ko_sentiment_clf = pipeline(\"sentiment-analysis\", model=\"sangrimlee/bert-base-multilingual-cased-nsmc\")\n",
    "\n",
    "ko_zero_shot_clf = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "candidate_labels=['기쁨', '칭찬', '죄송', '비판', '비난', '비꼼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb39d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = gr.State((False, []))  # (first_message_flag, chat_history)\n",
    "\n",
    "store = {}\n",
    "positive_count = 0\n",
    "situation_num = 0\n",
    "stack = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4567791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def chat(input_, situation_model_label, chat_model_label, state_in):\n",
    "    global store, positive_count, situation_num, stack, output_parser, model_map\n",
    "    global system_prompt, start_prompt, positive_prompt, negative_prompt\n",
    "    global user_input_situation, tone_summary_from_examples, candidate_labels\n",
    "    global ko_sentiment_clf, ko_zero_shot_clf\n",
    "\n",
    "    \n",
    "    # state unpacking\n",
    "    if state_in is None or not isinstance(state_in, list) or len(state_in) != 2:\n",
    "        state_in = [False, []]\n",
    "\n",
    "    first_message_flag, chat_history = state_in\n",
    "    image_path = \"images/img_negative.jpg\"\n",
    "\n",
    "    situation_model = model_map[situation_model_label]\n",
    "    chat_model = model_map[chat_model_label]\n",
    "\n",
    "    # 체인 구성\n",
    "    system_chain = system_prompt | situation_model | output_parser\n",
    "    start_chain = start_prompt | chat_model | output_parser\n",
    "    positive_chain = positive_prompt | chat_model | output_parser\n",
    "    negative_chain = negative_prompt | chat_model | output_parser\n",
    "\n",
    "    # 시작 메시지 처음 한 번만 출력\n",
    "    if not first_message_flag:\n",
    "        response = history_chain_runnable(start_chain, store).invoke(\n",
    "            input={\n",
    "                'query': \"\",\n",
    "                'user_input_situation': user_input_situation[0],\n",
    "                'tone_summary_from_examples': tone_summary_from_examples\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        chat_history.append(f\"상사: {response}\")\n",
    "        \n",
    "        return \"\\n\".join(chat_history), image_path, [True, chat_history]\n",
    "\n",
    "    # 감정 분류\n",
    "    input_status = ko_zero_shot_clf(input_, candidate_labels=candidate_labels)\n",
    "    input_status = input_status['labels'][:3]\n",
    "\n",
    "    if sum([item in candidate_labels[3:] for item in input_status]) >= 2 or input_status[0] == '죄송':\n",
    "        positive_count += 1\n",
    "\n",
    "    # 상황 변경 판단\n",
    "    system_msg = history_chain_runnable(system_chain, store).invoke(\n",
    "        input={\n",
    "            'query': \"\",\n",
    "            'user_input_situation': user_input_situation,\n",
    "            'conversation': format_conversation(store['LLM_game'])\n",
    "        },\n",
    "        config={'configurable': {'session_id': 'system_message'}}\n",
    "    )\n",
    "    if \"True\" in system_msg:\n",
    "        situation_num = min(situation_num + 1, len(user_input_situation) - 1)\n",
    "\n",
    "    # 응답 생성\n",
    "    if positive_count == 2:\n",
    "        response = history_chain_runnable(positive_chain, store).invoke(\n",
    "            input={\n",
    "                'user_input_situation': user_input_situation[situation_num],\n",
    "                'query': input_\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        image_path = \"images/img_positive.jpg\"\n",
    "        positive_count = 0\n",
    "    else:\n",
    "        response = history_chain_runnable(negative_chain, store).invoke(\n",
    "            input={\n",
    "                'user_input_situation': user_input_situation[situation_num],\n",
    "                'tone_summary_from_examples': tone_summary_from_examples,\n",
    "                'query': input_\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        image_path = \"images/img_negative.jpg\"\n",
    "\n",
    "    # 감정 평가 및 해고 여부\n",
    "    emotion = ko_sentiment_clf(response)\n",
    "    if emotion[0]['label'] == 'negative' and emotion[0]['score'] > 0.8:\n",
    "        stack += 1\n",
    "        image_path = \"images/img_angry.png\"\n",
    "\n",
    "    if stack >= 10:\n",
    "        chat_history.append(\"상사: =======당신은 해고되었습니다=======\")\n",
    "        return \"\\n\".join(chat_history), image_path, [first_message_flag, chat_history]\n",
    "\n",
    "    chat_history.append(f\"나: {input_}\\n상사: {response}\")\n",
    "    return \"\\n\".join(chat_history), image_path, [first_message_flag, chat_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4482e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swhong/.conda/envs/ollama_env/lib/python3.12/site-packages/gradio/components/base.py:203: UserWarning: 'scale' value should be an integer. Using 0.52 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://338f16f57b14d22159.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://338f16f57b14d22159.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_map = {\n",
    "    \"GPT-4o-mini (t=0.3)\": ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3),\n",
    "    \"GPT-4o (t=0.3)\": ChatOpenAI(model_name=\"gpt-4o\", temperature=0.3),\n",
    "    \"EEVE (t=0.3)\": ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.3),\n",
    "    \n",
    "    \"GPT-4o-mini (t=0.85)\": ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.85),\n",
    "    \"GPT-4o (t=0.85)\": ChatOpenAI(model_name=\"gpt-4o\", temperature=0.85),\n",
    "    \"EEVE (t=0.85)\": ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.85),\n",
    "}\n",
    "\n",
    "with gr.Blocks(title=\"💼 직장상사 응대 시뮬레이터\") as demo:\n",
    "    state = gr.State((False, []))\n",
    "\n",
    "    gr.Markdown(\"## 💬 직장상사에게 응답해 보세요!\")\n",
    "\n",
    "    # Input 영역 (상단 전체)\n",
    "    with gr.Row():\n",
    "        input_box = gr.Textbox(label=\"나:\", placeholder=\"상사에게 말하세요\", scale=3)\n",
    "        situation_radio = gr.Radio(\n",
    "            choices=[\"GPT-4o-mini (t=0.3)\", \"GPT-4o (t=0.3)\", \"EEVE (t=0.3)\"],\n",
    "            label=\"상황분석 모델 선택\", scale=1\n",
    "        )\n",
    "        chat_radio = gr.Radio(\n",
    "            choices=[\"GPT-4o-mini (t=0.85)\", \"GPT-4o (t=0.85)\", \"EEVE (t=0.85)\"],\n",
    "            label=\"대화 모델 선택\", scale=1\n",
    "        )\n",
    "\n",
    "    # Output 영역 (하단 2분할)\n",
    "    with gr.Row():\n",
    "        chat_output = gr.Textbox(label=\"대화 내역\", lines=20, scale=2)\n",
    "        image_output = gr.Image(label=\"상사의 표정\", type=\"filepath\", scale=0.52)\n",
    "\n",
    "    # 실행 버튼\n",
    "    run_btn = gr.Button(\"보내기\")\n",
    "\n",
    "    # 동작 연결\n",
    "    run_btn.click(\n",
    "        fn=chat,\n",
    "        inputs=[input_box, situation_radio, chat_radio, state],\n",
    "        outputs=[chat_output, image_output, state]\n",
    "    )\n",
    "\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
