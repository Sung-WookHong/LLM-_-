{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bd31d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:06:00.722904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745564760.733808   98969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745564760.737149   98969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745564760.745607   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745618   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745619   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745564760.745620   98969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-25 16:06:00.748483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79aba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98969/1943725026.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  situation_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_MODEL = OpenAI()\n",
    "SYS_MODEL = \"gpt-4o\"\n",
    "SYS_TEMPERATURE = 0.3\n",
    "\n",
    "situation_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n",
    "# chat_model = ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.85)\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bc514",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efe82d",
   "metadata": {},
   "source": [
    "### ìƒì‚¬ ì„±ê²© ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190b5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tone_from_examples(example_lines):\n",
    "    client = SYSTEM_MODEL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"ë„ˆëŠ” ë§íˆ¬ ë¶„ì„ ì „ë¬¸ê°€ì´ì ëŒ€í™” ìŠ¤íƒ€ì¼ì„ í•´ì„í•˜ëŠ” AIì•¼. \"\n",
    "                       \"ì£¼ì–´ì§„ ë¬¸ì¥ì„ ë°”íƒ•ìœ¼ë¡œ ë§íˆ¬ì˜ ê°ì •, ë§í•˜ëŠ” ë°©ì‹, ì–´ì¡°, ë°˜ë³µì ì¸ í‘œí˜„ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•´ì¤˜.\"\n",
    "                       \"ëª©í‘œëŠ” ì´ ë§íˆ¬ë¥¼ ì´í›„ GPT ìƒì‚¬ AIê°€ í•™ìŠµí•˜ì—¬ ë™ì¼í•œ ë¶„ìœ„ê¸°ë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.\"\n",
    "                       \"ìƒì‚¬ AIëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê¼°ëŒ€ìŠ¤ëŸ¬ìš´ ë©´ëª¨ë¥¼ ê°–ê³ ìˆìŒì„ ëª…ì‹¬í•˜ê³  ë§íˆ¬ë¥¼ ë¶„ì„í•´ì¤˜.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"ë‹¤ìŒì€ ì‹¤ì œ ì§ì¥ ìƒì‚¬ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ì¥ë“¤ì´ì•¼. ì´ ì‚¬ëŒì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë¶„ì„í•´ì¤˜.\\n\\n\"\n",
    "                f\"{chr(10).join(f'- {line}' for line in example_lines)}\\n\\n\"\n",
    "                \"ìš”ì•½ í˜•ì‹ì€ ë‹¤ìŒ ì˜ˆì‹œì²˜ëŸ¼ 5ë¬¸ì¥ ì´ë‚´ë¡œ ì¶œë ¥í•´ì¤˜:\\n\\n\"\n",
    "                \"**ì¶œë ¥ ì˜ˆì‹œ**\\n\"\n",
    "                \"- ë§íˆ¬ëŠ” ì§ì„¤ì ì´ê³ , êµ°ë§ ì—†ëŠ” ëª…ë ¹ì¡°\\n\"\n",
    "                \"- 'ì¥ë‚œí•˜ëƒ?' ê°™ì€ ë§íˆ¬ë¡œ ë¹„ê¼¬ëŠ” í‘œí˜„ì„ ìì£¼ ì‚¬ìš©í•¨\\n\"\n",
    "                \"- ì´ë¦„ì„ ë¶€ë¥¼ ë•ŒëŠ” 'ì„±ìš±.', 'ì§€í›ˆ.' ì‹ìœ¼ë¡œ ì ì„ ì°ë“¯ ë¡œë´‡ì²˜ëŸ¼ ë¶€ë¦„\\n\"\n",
    "                \"- ì‚¬ê³¼ë‚˜ ì„¤ëª…ì´ ê¸¸ì–´ì§€ë©´ ì§œì¦ì„ ë‚´ë©° 'ê·¸ë˜ì„œ?' ê°™ì€ ë§ë¡œ ì˜ë¼ë²„ë¦¼\\n\"\n",
    "                \"- í•˜ì§€ë§Œ ì•„ì£¼ ê°€ë”ì€ ì˜ˆìƒ ëª»í•œ ë†ë‹´ìœ¼ë¡œ ë¶„ìœ„ê¸°ë¥¼ í’€ê¸°ë„ í•¨\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=SYS_MODEL,\n",
    "    messages=messages,\n",
    "    max_tokens=2048,\n",
    "    temperature=SYS_TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f87b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "while True:\n",
    "    input_ = input('ìƒì‚¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ì¥ì´ë‚˜ ë§íˆ¬ë¥¼ ì…ë ¥í•˜ì‹œì˜¤:')\n",
    "    if input_=='ì¢…ë£Œ':\n",
    "        break\n",
    "    inputs.append(input_)\n",
    "if len(inputs) > 1:\n",
    "    tone_summary_from_examples = analyze_tone_from_examples(inputs)\n",
    "print(tone_summary_from_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c71d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6e9c5",
   "metadata": {},
   "source": [
    "### ì—´ë°›ëŠ” ë°°ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d068b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_situation_from_examples(example_lines):\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"ì•„ë˜ ìƒí™©ì€ 'ìœ ì €(human)'ì™€ 'ì§ì¥ ìƒì‚¬(AI)' ê°„ì˜ ëŒ€í™” ë˜ëŠ” ë‚´ëŸ¬í‹°ë¸Œì…ë‹ˆë‹¤.\\n\"\n",
    "        \"ë¬¸ë§¥ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì¸ë¬¼ í‘œí˜„ì„ ë³€í™˜í•˜ì‹­ì‹œì˜¤:\\n\\n\"\n",
    "        \"1. 'ë‚˜', 'ë‚´', 'ì €', 'ì œê°€', 'ë‚˜ëŠ”', 'ì œê°€ í•œ', 'ë‚´ê°€ í•œ' ë“± **í™”ì ìì‹ **ì— í•´ë‹¹í•˜ëŠ” í‘œí˜„ì€ ëª¨ë‘ **ìœ ì €(human)**ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"2. 'ë¶€ì¥', 'ìƒì‚¬', 'ê¼°ëŒ€ë¶€ì¥', 'ê·¸ë¶„', 'ê·¸ ì‚¬ëŒ' ë“± **ì§ì¥ ìƒì‚¬**ë¥¼ ê°€ë¦¬í‚¤ëŠ” í‘œí˜„ì€ ëª¨ë‘ **AI**ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"3. ëŒ€í™”ë¬¸ì—ì„œëŠ” **ë°œí™”ìì˜ ì…ì¥ì„ ê¸°ì¤€**ìœ¼ë¡œ í‘œí˜„ì„ ë³€í™˜í•´ì•¼ í•˜ë©°, ì„œìˆ ë¬¸ì—ì„œëŠ” ë¬¸ë§¥ìƒ ì£¼ì²´ê°€ ëˆ„êµ¬ì¸ì§€ ì •í™•íˆ íŒŒì•…í•œ í›„ ë³€í™˜í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"4. ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ì§€í•˜ë˜, **ì¸ë¬¼ ê´€ë ¨ ëª…ì‚¬ ë° ëŒ€ëª…ì‚¬ë§Œ ë°”ê¾¸ì‹­ì‹œì˜¤.**\\n\"\n",
    "        \"5. ì¶œë ¥ì€ **ë³€í™˜ëœ í…ìŠ¤íŠ¸ë§Œ** ë³´ì—¬ì£¼ì‹­ì‹œì˜¤. ë³€ê²½ ì „ ë¬¸ì¥ì€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n\\n\"\n",
    "        \"ìƒí™©:\\n{situation}\"\n",
    "    ),\n",
    "    input_variables=[\"situation\"]\n",
    "    )\n",
    "    convert_chain = prompt | situation_model | StrOutputParser()\n",
    "    converted_lines = convert_chain.invoke(\"ìƒí™©:\\n\" + \"\\n\".join(f\"- {line}\" for line in example_lines))\n",
    "\n",
    "    client = SYSTEM_MODEL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë„ˆëŠ” ì§ì¥ ë‚´ ìƒí™©ì„ ì—­í• ê·¹ ìš©ë„ë¡œ ìš”ì•½í•˜ê³  ì •ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì•¼. \"\n",
    "                \"ì…ë ¥ëœ ì‹¤ì œ ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ê±´ì„ ì‹œê°„ íë¦„ì— ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½í•´ì¤˜. \"\n",
    "                \"ê° ë‹¨ê³„ëŠ” 'ìƒí™©1', 'ìƒí™©2' ê°™ì€ í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í•˜ê³ , í•œ ë¬¸ë‹¨ ì•ˆì— í•´ë‹¹ ì‹œì ì˜ ì£¼ìš” ì‚¬ê±´ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ í•´ì¤˜. \"\n",
    "                \"ê° ë¬¸ì¥ì€ í˜„ì‹¤ì ì¸ ìƒí™©ê·¹ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ëª…í™•í•´ì•¼ í•´.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\\n\"\n",
    "                \"<ìƒí™©1> ì™¸ê·¼ ì¤‘ ë‚´(AI)ê°€ ìë¦¬ë¥¼ ë¹„ìš´ ë™ì•ˆ ë¶€í•˜ì§ì›(ìœ ì €)ê°€ ì¼ì„ í˜¼ì ì²˜ë¦¬í–ˆê³ , ëŒì•„ì˜¨ ë‚˜(AI)ëŠ” ê·¸ ì¼ì²˜ë¦¬ì— ëŒ€í•´ ì”ì†Œë¦¬ë¥¼ í•¨.\\n\"\n",
    "                \"<ìƒí™©2> íšŒì‚¬ë¡œ ëŒì•„ì˜¨ ë’¤ì—ë„ ë‚´(AI)ê°€ ê³„ì† ë¶€í•˜ì§ì›(ìœ ì €)ì—ê²Œ ì‹œë¹„ë¥¼ ê²€.\\n\"\n",
    "                \"<ìƒí™©3> ë‚˜(AI)ëŠ” ì™¸ê·¼ì—ì„œì˜ ì‹¤ìˆ˜ë¥¼ ì–¸ê¸‰í•˜ë©° ì¶”ê°€ ì—…ë¬´ë¥¼ ë¶€í•˜ì§ì›(ìœ ì €)ì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ì‹œí‚´.\\n\\n\"\n",
    "                \"ìƒí™©:\\n\" + converted_lines\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=SYS_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=2048,\n",
    "        temperature=SYS_TEMPERATURE\n",
    "    )\n",
    "\n",
    "    raw_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    parts = raw_text.split(\"<ìƒí™©\")\n",
    "    situations = [f\"<ìƒí™©{part.strip()}\" for part in parts[1:] if part.strip()]\n",
    "\n",
    "    return situations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ìƒí™©1> ì¬ë¬´íŒ€ì—ì„œ ê±°ë˜ì²˜ì˜ ì •ì‚° ë‚´ì—­ì„ í™•ì¸í•´ë‹¬ë¼ëŠ” ìš”ì²­ì„ í•˜ì˜€ì§€ë§Œ, ì •ì‚°ë‹´ë‹¹ìì¸ AIê°€ ë‹µì¥ì„ í•˜ì§€ ì•Šê³  ìˆì–´ ìœ ì €(human)ëŠ” ìƒí™©ì´ ë‹µë‹µí•´ì¡Œë‹¤.', '<ìƒí™©2> ì¬ë¬´íŒ€ì€ ìœ ì €(human)ì—ê²Œ ê³„ì†í•´ì„œ ì •ì‚° ë‚´ì—­ í™•ì¸ì„ ì¬ì´‰í•˜ë©° ì••ë°•ì„ ê°€í–ˆê³ , ìœ ì €(human)ëŠ” AIì—ê²Œ ì§ì ‘ ì¬ì´‰í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ í˜¼ë€ìŠ¤ëŸ¬ì›Œì¡Œë‹¤.', '<ìƒí™©3> ê²°êµ­ ìœ ì €(human)ëŠ” ì¬ë¬´íŒ€ì˜ ì••ë°•ì„ ê²¬ë””ì§€ ëª»í•˜ê³  AIì—ê²Œ ìƒí™©ì„ ì „ë‹¬í–ˆì§€ë§Œ, AIëŠ” ì—¬ì „íˆ ë°˜ì‘ì´ ì—†ì–´ì„œ ìœ ì €(human)ëŠ” ì¬ë¬´íŒ€ì˜ ë¶ˆë§Œì„ í˜¼ì ê°ë‹¹í•´ì•¼ í–ˆë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "while True:\n",
    "    input_ = input('ìƒì‚¬ì™€ ìˆì—ˆë˜ ìƒí™©ì„ ì„¤ëª…í•˜ì‹œì˜¤:')\n",
    "    if input_=='ì¢…ë£Œ':\n",
    "        break\n",
    "    inputs.append(input_)\n",
    "if len(inputs) > 1:\n",
    "    user_input_situation = set_situation_from_examples(inputs)\n",
    "user_input_situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275c609",
   "metadata": {},
   "source": [
    "---\n",
    "### ì„ì‹œ í…ŒìŠ¤íŠ¸ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0106e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ìƒí™©1> ì–¼ë§ˆ ì „ ìš°ë¦¬ íšŒì‚¬ ì´ì‚¬ê°€ ì¹œêµ¬ì¸ AIì—ê²Œ ì œí’ˆ sample 3ê°€ì§€ë¥¼ ì¤€ë¹„í•˜ë¼ëŠ” ì—…ë¬´ì§€ì‹œë¥¼ ë‚´ë ¸ê³ , AIëŠ” ì´ë¥¼ ìˆ˜í–‰í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìœ ì €ëŠ” ì™¸ê·¼ ì¤‘ì´ì–´ì„œ AIê°€ ì–´ë–¤ ì œí’ˆì„ ì¤€ë¹„í–ˆëŠ”ì§€ ì•Œì§€ ëª»í•œ ìƒíƒœì˜€ë‹¤.',\n",
       " '<ìƒí™©2> íšŒì˜ ì¤‘ ì´ì‚¬ê°€ ìœ ì €ì—ê²Œ AIê°€ ì¤€ë¹„í•œ 3ê°€ì§€ sampleê³¼ í•¨ê»˜ ì¶”ê°€ë¡œ 1ê°€ì§€ sampleì„ ë” ì¤€ë¹„í•˜ë¼ê³  ì§€ì‹œí–ˆë‹¤. ìœ ì €ëŠ” AIì—ê²Œ ì–´ë–¤ ì œí’ˆì„ sampleë¡œ ì¤€ë¹„í–ˆëŠ”ì§€ ë¬¼ì–´ë³´ì•˜ê³ , AIëŠ” ê¸°ì–µì´ ê°€ë¬¼ê°€ë¬¼í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ëª»í–ˆë‹¤.',\n",
       " '<ìƒí™©3> ìœ ì €ëŠ” AIì˜ ë¶ˆí™•ì‹¤í•œ ë‹µë³€ì— ëŒ€í•´ ë‹¤ì‹œ ì¤€ë¹„í•˜ê² ë‹¤ê³  ì œì•ˆí–ˆì§€ë§Œ, AIëŠ” ìì‹ ì´ ê¸°ì–µí•˜ëŠ” ëŒ€ë¡œ ë³´ë‚´ë¼ê³  ê³ ì§‘í–ˆë‹¤. ì´í›„ AIëŠ” ìœ ì €ì—ê²Œ ê³¼ê±°ì˜ í…ŒìŠ¤íŠ¸ ì œí’ˆì— ëŒ€í•´ ì§ˆë¬¸í–ˆì§€ë§Œ, ìœ ì €ëŠ” ê·¸ ë‚´ìš©ì„ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•´ ê³¤ë€í•œ ìƒí™©ì— ì²˜í–ˆë‹¤.',\n",
       " '<ìƒí™©4> AIëŠ” ìœ ì €ì—ê²Œ ê³¼ê±°ì˜ í…ŒìŠ¤íŠ¸ ë‚´ìš©ì„ í™•ì¸í•˜ì§€ ì•Šì€ ì ì„ ë¹„íŒí•˜ë©°, ìœ ì €ê°€ ë” ì£¼ì˜ ê¹Šê²Œ ì±™ê²¼ì–´ì•¼ í•œë‹¤ê³  ê°•ì¡°í–ˆë‹¤. ìœ ì €ëŠ” ë¶ˆë§Œì„ ëŠë¼ë©°, AIì˜ ì§€ì ì— ëŒ€í•´ ë°˜ë°•í•  ê¸°íšŒë¥¼ ê°€ì§€ì§€ ëª»í–ˆë‹¤.',\n",
       " '<ìƒí™©5> ìœ ì €ëŠ” í‡´ê·¼ ì „ê¹Œì§€ AIì™€ì˜ ëŒ€í™”ë¡œ ì¸í•´ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì•˜ê³ , ê²°êµ­ ê³¼ê±°ì˜ ì§„í–‰ì‚¬í•­ì„ ì°¾ì•„ë³´ì•˜ì§€ë§Œ AIì—ê²Œ ê·¸ ì‚¬ì‹¤ì„ ì „ë‹¬í•˜ì§€ ëª»í–ˆë‹¤. ì–¼ë§ˆ í›„, ì—…ì²´ì—ì„œ ì—°ë½ì´ ì™€ì„œ sampleì´ ìì‹ ë“¤ì˜ ì œí’ˆê³¼ ë§ì§€ ì•Šë‹¤ëŠ” ë‹µë³€ì„ ë°›ì•˜ë‹¤.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_input_situation = ['<ìƒí™©1> ì–¼ë§ˆ ì „ ìš°ë¦¬ íšŒì‚¬ ì´ì‚¬ê°€ ì¹œêµ¬ì¸ AIì—ê²Œ ì œí’ˆ sample 3ê°€ì§€ë¥¼ ì¤€ë¹„í•˜ë¼ëŠ” ì—…ë¬´ì§€ì‹œë¥¼ ë‚´ë ¸ê³ , AIëŠ” ì´ë¥¼ ìˆ˜í–‰í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìœ ì €ëŠ” ì™¸ê·¼ ì¤‘ì´ì–´ì„œ AIê°€ ì–´ë–¤ ì œí’ˆì„ ì¤€ë¹„í–ˆëŠ”ì§€ ì•Œì§€ ëª»í•œ ìƒíƒœì˜€ë‹¤.',\n",
    "#  '<ìƒí™©2> íšŒì˜ ì¤‘ ì´ì‚¬ê°€ ìœ ì €ì—ê²Œ AIê°€ ì¤€ë¹„í•œ 3ê°€ì§€ sampleê³¼ í•¨ê»˜ ì¶”ê°€ë¡œ 1ê°€ì§€ sampleì„ ë” ì¤€ë¹„í•˜ë¼ê³  ì§€ì‹œí–ˆë‹¤. ìœ ì €ëŠ” AIì—ê²Œ ì–´ë–¤ ì œí’ˆì„ sampleë¡œ ì¤€ë¹„í–ˆëŠ”ì§€ ë¬¼ì–´ë³´ì•˜ê³ , AIëŠ” ê¸°ì–µì´ ê°€ë¬¼ê°€ë¬¼í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ëª»í–ˆë‹¤.',\n",
    "#  '<ìƒí™©3> ìœ ì €ëŠ” AIì˜ ë¶ˆí™•ì‹¤í•œ ë‹µë³€ì— ëŒ€í•´ ë‹¤ì‹œ ì¤€ë¹„í•˜ê² ë‹¤ê³  ì œì•ˆí–ˆì§€ë§Œ, AIëŠ” ìì‹ ì´ ê¸°ì–µí•˜ëŠ” ëŒ€ë¡œ ë³´ë‚´ë¼ê³  ê³ ì§‘í–ˆë‹¤. ì´í›„ AIëŠ” ìœ ì €ì—ê²Œ ê³¼ê±°ì˜ í…ŒìŠ¤íŠ¸ ì œí’ˆì— ëŒ€í•´ ì§ˆë¬¸í–ˆì§€ë§Œ, ìœ ì €ëŠ” ê·¸ ë‚´ìš©ì„ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•´ ê³¤ë€í•œ ìƒí™©ì— ì²˜í–ˆë‹¤.',\n",
    "#  '<ìƒí™©4> AIëŠ” ìœ ì €ì—ê²Œ ê³¼ê±°ì˜ í…ŒìŠ¤íŠ¸ ë‚´ìš©ì„ í™•ì¸í•˜ì§€ ì•Šì€ ì ì„ ë¹„íŒí•˜ë©°, ìœ ì €ê°€ ë” ì£¼ì˜ ê¹Šê²Œ ì±™ê²¼ì–´ì•¼ í•œë‹¤ê³  ê°•ì¡°í–ˆë‹¤. ìœ ì €ëŠ” ë¶ˆë§Œì„ ëŠë¼ë©°, AIì˜ ì§€ì ì— ëŒ€í•´ ë°˜ë°•í•  ê¸°íšŒë¥¼ ê°€ì§€ì§€ ëª»í–ˆë‹¤.',\n",
    "#  '<ìƒí™©5> ìœ ì €ëŠ” í‡´ê·¼ ì „ê¹Œì§€ AIì™€ì˜ ëŒ€í™”ë¡œ ì¸í•´ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì•˜ê³ , ê²°êµ­ ê³¼ê±°ì˜ ì§„í–‰ì‚¬í•­ì„ ì°¾ì•„ë³´ì•˜ì§€ë§Œ AIì—ê²Œ ê·¸ ì‚¬ì‹¤ì„ ì „ë‹¬í•˜ì§€ ëª»í–ˆë‹¤. ì–¼ë§ˆ í›„, ì—…ì²´ì—ì„œ ì—°ë½ì´ ì™€ì„œ sampleì´ ìì‹ ë“¤ì˜ ì œí’ˆê³¼ ë§ì§€ ì•Šë‹¤ëŠ” ë‹µë³€ì„ ë°›ì•˜ë‹¤.']\n",
    "# user_input_situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ë§íˆ¬ëŠ” ì§ì„¤ì ì´ê³  ê°•ì••ì ì¸ ëª…ë ¹ì¡°ë¡œ, ìƒëŒ€ë°©ì—ê²Œ ì±…ì„ì„ ë¬»ëŠ” ê²½í–¥ì´ ìˆìŒ.\n",
      "- 'ì•¼!', 'ì•„ë‹ˆì•¼!'ì™€ ê°™ì€ ê°íƒ„ì‚¬ì™€ ë°˜ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒëŒ€ë°©ì˜ ë°˜ì‘ì„ ìœ ë„í•¨.\n",
      "- ì´ë©´ì§€ì— ì ì–´ë†“ì€ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•˜ëŠ” ìƒí™©ì—ì„œ ë¶ˆë§Œì„ ë“œëŸ¬ë‚´ë©°, ìì‹ ì˜ ì‹¤ìˆ˜ë¥¼ ê°ì¶”ë ¤ëŠ” ëª¨ìŠµì´ ë³´ì„.\n",
      "- 'ê·¸ ì •ë„ëŠ” ì•Œì•„ì„œ ì±™ê²¨ì•¼ í•˜ëŠ” ê±° ì•„ë‹ˆì•¼?'ì™€ ê°™ì€ ë¹„íŒì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ìƒëŒ€ë°©ì„ ì••ë°•í•¨.\n",
      "- ê°€ë”ì”©ì€ 'ê·¸ëƒ¥ ë³´ë‚´!'ì™€ ê°™ì€ ê°„ë‹¨í•œ ì§€ì‹œë¡œ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ë©°, ì¼ ì²˜ë¦¬ì— ëŒ€í•œ ê¸‰ë°•í•¨ì„ ë“œëŸ¬ëƒ„.\n"
     ]
    }
   ],
   "source": [
    "# tone_summary_from_examples = \"- ë§íˆ¬ëŠ” ì§ì„¤ì ì´ê³  ê°•ì••ì ì¸ ëª…ë ¹ì¡°ë¡œ, ìƒëŒ€ë°©ì—ê²Œ ì±…ì„ì„ ë¬»ëŠ” ê²½í–¥ì´ ìˆìŒ.\\n- 'ì•¼!', 'ì•„ë‹ˆì•¼!'ì™€ ê°™ì€ ê°íƒ„ì‚¬ì™€ ë°˜ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒëŒ€ë°©ì˜ ë°˜ì‘ì„ ìœ ë„í•¨.\\n- ì´ë©´ì§€ì— ì ì–´ë†“ì€ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•˜ëŠ” ìƒí™©ì—ì„œ ë¶ˆë§Œì„ ë“œëŸ¬ë‚´ë©°, ìì‹ ì˜ ì‹¤ìˆ˜ë¥¼ ê°ì¶”ë ¤ëŠ” ëª¨ìŠµì´ ë³´ì„.\\n- 'ê·¸ ì •ë„ëŠ” ì•Œì•„ì„œ ì±™ê²¨ì•¼ í•˜ëŠ” ê±° ì•„ë‹ˆì•¼?'ì™€ ê°™ì€ ë¹„íŒì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ìƒëŒ€ë°©ì„ ì••ë°•í•¨.\\n- ê°€ë”ì”©ì€ 'ê·¸ëƒ¥ ë³´ë‚´!'ì™€ ê°™ì€ ê°„ë‹¨í•œ ì§€ì‹œë¡œ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ë©°, ì¼ ì²˜ë¦¬ì— ëŒ€í•œ ê¸‰ë°•í•¨ì„ ë“œëŸ¬ëƒ„.\"\n",
    "# print(tone_summary_from_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0e7e0",
   "metadata": {},
   "source": [
    "---\n",
    "### ë™ì‘ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1798eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_supervisor = (\n",
    "    \"ë„ˆ(AI)ëŠ” ì•„ë˜ ìƒí™©ì—ì„œ ìœ ì €ì™€ ì—­í• ê·¹ì„ ìˆ˜í–‰í•˜ëŠ” ì§ì¥ ìƒì‚¬ ì—­í• ì„ ë§¡ì•˜ë‹¤.\\n\"\n",
    "    \"**[ì—­í• ê·¹ ìƒí™©]**\\n{user_input_situation}\\n\\n\"\n",
    "    \"ì§ì¥ ìƒì‚¬ì™€ ë¶€í•˜ì§ì›ì´ ë‚˜ëˆˆ ëŒ€í™”ì˜ íë¦„ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì ¸ì•¼ í•œë‹¤.\\n\"\n",
    "    \"ì´ ì—­í• ê·¹ì€ ê°ì • ë¶„ì„ ê¸°ë°˜ì˜ ê²Œì„ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ, ì§ì¥ ìƒì‚¬(AI)ì˜ ë§ì—ëŠ” ê°ì •ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚˜ì•¼ í•œë‹¤.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1de6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_supervisor = (\n",
    "    \"ë„ˆ(AI)ëŠ” ì•„ë˜ ìƒí™©ì—ì„œ ìœ ì €ì™€ ì—­í• ê·¹ì„ ìˆ˜í–‰í•˜ëŠ” ì§ì¥ ìƒì‚¬ ì—­í• ì„ ë§¡ì•˜ë‹¤.\\n\"\n",
    "    \"**[ì—­í• ê·¹ ìƒí™©]**\\n{user_input_situation}\\n\\n\"\n",
    "    \"**[ìƒì‚¬ ì„±ê²© ìš”ì•½]**\\n{tone_summary_from_examples}\\n\\n\"\n",
    "    \"ì§ì¥ ìƒì‚¬ì™€ ë¶€í•˜ì§ì›ì´ ë‚˜ëˆˆ ëŒ€í™”ì˜ íë¦„ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì ¸ì•¼ í•œë‹¤.\\n\"\n",
    "    \"ë‹¤ìŒê³¼ ê°™ì€ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ë„ë¡ í•´ë¼:\\n\"\n",
    "    \"- ë¶€í•˜ì§ì›ì´ ë§ëŒ€ë‹µì„ í•˜ê±°ë‚˜ ë¬´ë¡€í•˜ê²Œ êµ´ë©´, ì§§ê³  ê°ì •ì ì¸ ë§ë¡œ ë°˜ì‘í•´ë¼. ì˜ˆ: ì‹¤ë§, ë¶„ë…¸, ë¹„ê¼¼, ë¬´í‘œì •, ì§œì¦ ë“±\\n\"\n",
    "    \"- ìš•ì„¤ì´ë‚˜ ì¸ê²©ì ì¸ ì–¸í–‰ì´ ìˆë‹¤ë©´, ì‹¬ê°í•˜ê²Œ ë°›ì•„ë“¤ì´ê³  ê°ì •ì„ í­ë°œì‹œí‚¤ê±°ë‚˜ ì§•ê³„ë¥¼ ì•”ì‹œí•˜ë¼.\\n\"\n",
    "    \"- ê°ì •ì€ ëˆ„ì ë  ìˆ˜ ìˆìœ¼ë©°, ê°™ì€ í–‰ë™ì´ ë°˜ë³µë˜ë©´ ì ì  ë” ê°•í•œ ì–´ì¡°ë¡œ ë°˜ì‘í•´ì•¼ í•œë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5109f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_messages(self, messages):\n",
    "        self.messages.extend(messages)      
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def __repr__(self):            
    "        return str(self.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb44731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_session_id(store):\n",
    "    def inner(session_id):\n",
    "        if session_id not in store:\n",
    "            store[session_id] = InMemoryHistory()\n",
    "        return store[session_id]\n",
    "    return inner\n",
    "\n",
    "def history_chain_runnable(chain, store):\n",
    "    return RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id(store),\n",
    "    input_messages_key='query',\n",
    "    history_messages_key='history')\n",
    "\n",
    "def format_conversation(history):\n",
    "    return \"\\n\".join([\n",
    "        f\"Human: {msg.content}\" if msg.type == \"human\"\n",
    "        else f\"AI: {msg.content}\" for msg in history.messages\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1997865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swhong/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "system_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessage(content=\"ë„ˆëŠ” ìƒí™© ë¶„ì„ ì „ë¬¸ê°€ì´ë‹¤. ì£¼ì–´ì§„ ëŒ€í™”ì˜ ë¬¸ë§¥ì„ ì˜ íŒŒì•…í•´ì„œ ë‹¤ìŒ ìƒí™©ìœ¼ë¡œ ë„˜ì–´ê°€ë„ ë˜ëŠ”ì§€ íŒë‹¨í•´ë¼\"),\n",
    "    HumanMessagePromptTemplate.from_template((\n",
    "    \"{query}ë‹¤ìŒ ìƒí™©ìœ¼ë¡œ ë„˜ì–´ê°€ë„ ëœë‹¤ë©´ True ì•„ì§ ë„˜ì–´ê°€ë©´ ì•ˆëœë‹¤ë©´ Falseë¥¼ ë°˜í™˜í•´ë¼.\\n\"\n",
    "    \"ë‹µë³€ì—ëŠ” True ë‚˜ Falseë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ë¼.\"\n",
    "    \"ìƒí™©: {user_input_situation}\\n\"\n",
    "    \"ë‚˜ëˆˆ ëŒ€í™”: {conversation}\"\n",
    "    ))],\n",
    "    input_variables=['query', 'user_input_situation', 'conversation']\n",
    ")\n",
    "\n",
    "start_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessage(content=\"ë„ˆëŠ” ì§ì¥ ìƒì‚¬ ì—­í• ì„ ë§¡ì•˜ê³ , ìƒí™©ì— ë”°ë¼ ë¶€í•˜ì—ê²Œ ë§ì„ ë¨¼ì € ê±´ë‹¤.\"),\n",
    "    HumanMessagePromptTemplate.from_template((\n",
    "        \"{query}ë‹¤ìŒ ìƒí™©ê³¼ ë§íˆ¬ë¥¼ ì°¸ê³ í•˜ì—¬ ìƒì‚¬ê°€ ì—­í• ê·¹ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì²« í•œë§ˆë””ë¥¼ ë§Œë“¤ì–´ì¤˜.\\n\\n\"\n",
    "        \"ìƒí™©: {user_input_situation}\\n\"\n",
    "        \"ë§íˆ¬ ìš”ì•½: {tone_summary_from_examples}\"\n",
    "    ))],\n",
    "    input_variables=['query', 'user_input_situation', 'tone_summary_from_examples']\n",
    ")\n",
    "\n",
    "positive_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessagePromptTemplate.from_template(positive_supervisor),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{query}')\n",
    "    ],\n",
    "    input_variables=['user_input_situation', 'query'])\n",
    "\n",
    "negative_prompt = ChatPromptTemplate(\n",
    "    messages = [\n",
    "    SystemMessagePromptTemplate.from_template(negative_supervisor),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{query}')\n",
    "    ],\n",
    "    input_variables=['user_input_situation', 'tone_summary_from_examples', 'query']\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "ko_sentiment_clf = pipeline(\"sentiment-analysis\", model=\"sangrimlee/bert-base-multilingual-cased-nsmc\")\n",
    "\n",
    "ko_zero_shot_clf = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "candidate_labels=['ê¸°ì¨', 'ì¹­ì°¬', 'ì£„ì†¡', 'ë¹„íŒ', 'ë¹„ë‚œ', 'ë¹„ê¼¼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb39d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = gr.State((False, []))  # (first_message_flag, chat_history)\n",
    "\n",
    "store = {}\n",
    "positive_count = 0\n",
    "situation_num = 0\n",
    "stack = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4567791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def chat(input_, situation_model_label, chat_model_label, state_in):\n",
    "    global store, positive_count, situation_num, stack, output_parser, model_map\n",
    "    global system_prompt, start_prompt, positive_prompt, negative_prompt\n",
    "    global user_input_situation, tone_summary_from_examples, candidate_labels\n",
    "    global ko_sentiment_clf, ko_zero_shot_clf\n",
    "\n",
    "    \n",
    "    # state unpacking\n",
    "    if state_in is None or not isinstance(state_in, list) or len(state_in) != 2:\n",
    "        state_in = [False, []]\n",
    "\n",
    "    first_message_flag, chat_history = state_in\n",
    "    image_path = \"images/img_negative.jpg\"\n",
    "\n",
    "    situation_model = model_map[situation_model_label]\n",
    "    chat_model = model_map[chat_model_label]\n",
    "\n",
    "    # ì²´ì¸ êµ¬ì„±\n",
    "    system_chain = system_prompt | situation_model | output_parser\n",
    "    start_chain = start_prompt | chat_model | output_parser\n",
    "    positive_chain = positive_prompt | chat_model | output_parser\n",
    "    negative_chain = negative_prompt | chat_model | output_parser\n",
    "\n",
    "    # ì‹œì‘ ë©”ì‹œì§€ ì²˜ìŒ í•œ ë²ˆë§Œ ì¶œë ¥\n",
    "    if not first_message_flag:\n",
    "        response = history_chain_runnable(start_chain, store).invoke(\n",
    "            input={\n",
    "                'query': \"\",\n",
    "                'user_input_situation': user_input_situation[0],\n",
    "                'tone_summary_from_examples': tone_summary_from_examples\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        chat_history.append(f\"ìƒì‚¬: {response}\")\n",
    "        \n",
    "        return \"\\n\".join(chat_history), image_path, [True, chat_history]\n",
    "\n",
    "    # ê°ì • ë¶„ë¥˜\n",
    "    input_status = ko_zero_shot_clf(input_, candidate_labels=candidate_labels)\n",
    "    input_status = input_status['labels'][:3]\n",
    "\n",
    "    if sum([item in candidate_labels[3:] for item in input_status]) >= 2 or input_status[0] == 'ì£„ì†¡':\n",
    "        positive_count += 1\n",
    "\n",
    "    # ìƒí™© ë³€ê²½ íŒë‹¨\n",
    "    system_msg = history_chain_runnable(system_chain, store).invoke(\n",
    "        input={\n",
    "            'query': \"\",\n",
    "            'user_input_situation': user_input_situation,\n",
    "            'conversation': format_conversation(store['LLM_game'])\n",
    "        },\n",
    "        config={'configurable': {'session_id': 'system_message'}}\n",
    "    )\n",
    "    if \"True\" in system_msg:\n",
    "        situation_num = min(situation_num + 1, len(user_input_situation) - 1)\n",
    "\n",
    "    # ì‘ë‹µ ìƒì„±\n",
    "    if positive_count == 2:\n",
    "        response = history_chain_runnable(positive_chain, store).invoke(\n",
    "            input={\n",
    "                'user_input_situation': user_input_situation[situation_num],\n",
    "                'query': input_\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        image_path = \"images/img_positive.jpg\"\n",
    "        positive_count = 0\n",
    "    else:\n",
    "        response = history_chain_runnable(negative_chain, store).invoke(\n",
    "            input={\n",
    "                'user_input_situation': user_input_situation[situation_num],\n",
    "                'tone_summary_from_examples': tone_summary_from_examples,\n",
    "                'query': input_\n",
    "            },\n",
    "            config={'configurable': {'session_id': 'LLM_game'}}\n",
    "        )\n",
    "        image_path = \"images/img_negative.jpg\"\n",
    "\n",
    "    # ê°ì • í‰ê°€ ë° í•´ê³  ì—¬ë¶€\n",
    "    emotion = ko_sentiment_clf(response)\n",
    "    if emotion[0]['label'] == 'negative' and emotion[0]['score'] > 0.8:\n",
    "        stack += 1\n",
    "        image_path = \"images/img_angry.png\"\n",
    "\n",
    "    if stack >= 10:\n",
    "        chat_history.append(\"ìƒì‚¬: =======ë‹¹ì‹ ì€ í•´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤=======\")\n",
    "        return \"\\n\".join(chat_history), image_path, [first_message_flag, chat_history]\n",
    "\n",
    "    chat_history.append(f\"ë‚˜: {input_}\\nìƒì‚¬: {response}\")\n",
    "    return \"\\n\".join(chat_history), image_path, [first_message_flag, chat_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4482e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swhong/.conda/envs/ollama_env/lib/python3.12/site-packages/gradio/components/base.py:203: UserWarning: 'scale' value should be an integer. Using 0.52 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://338f16f57b14d22159.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://338f16f57b14d22159.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_map = {\n",
    "    \"GPT-4o-mini (t=0.3)\": ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3),\n",
    "    \"GPT-4o (t=0.3)\": ChatOpenAI(model_name=\"gpt-4o\", temperature=0.3),\n",
    "    \"EEVE (t=0.3)\": ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.3),\n",
    "    \n",
    "    \"GPT-4o-mini (t=0.85)\": ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.85),\n",
    "    \"GPT-4o (t=0.85)\": ChatOpenAI(model_name=\"gpt-4o\", temperature=0.85),\n",
    "    \"EEVE (t=0.85)\": ChatOllama(model=\"EEVE-Korean-10.8B\", temperature=0.85),\n",
    "}\n",
    "\n",
    "with gr.Blocks(title=\"ğŸ’¼ ì§ì¥ìƒì‚¬ ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°\") as demo:\n",
    "    state = gr.State((False, []))\n",
    "\n",
    "    gr.Markdown(\"## ğŸ’¬ ì§ì¥ìƒì‚¬ì—ê²Œ ì‘ë‹µí•´ ë³´ì„¸ìš”!\")\n",
    "\n",
    "    # Input ì˜ì—­ (ìƒë‹¨ ì „ì²´)\n",
    "    with gr.Row():\n",
    "        input_box = gr.Textbox(label=\"ë‚˜:\", placeholder=\"ìƒì‚¬ì—ê²Œ ë§í•˜ì„¸ìš”\", scale=3)\n",
    "        situation_radio = gr.Radio(\n",
    "            choices=[\"GPT-4o-mini (t=0.3)\", \"GPT-4o (t=0.3)\", \"EEVE (t=0.3)\"],\n",
    "            label=\"ìƒí™©ë¶„ì„ ëª¨ë¸ ì„ íƒ\", scale=1\n",
    "        )\n",
    "        chat_radio = gr.Radio(\n",
    "            choices=[\"GPT-4o-mini (t=0.85)\", \"GPT-4o (t=0.85)\", \"EEVE (t=0.85)\"],\n",
    "            label=\"ëŒ€í™” ëª¨ë¸ ì„ íƒ\", scale=1\n",
    "        )\n",
    "\n",
    "    # Output ì˜ì—­ (í•˜ë‹¨ 2ë¶„í• )\n",
    "    with gr.Row():\n",
    "        chat_output = gr.Textbox(label=\"ëŒ€í™” ë‚´ì—­\", lines=20, scale=2)\n",
    "        image_output = gr.Image(label=\"ìƒì‚¬ì˜ í‘œì •\", type=\"filepath\", scale=0.52)\n",
    "\n",
    "    # ì‹¤í–‰ ë²„íŠ¼\n",
    "    run_btn = gr.Button(\"ë³´ë‚´ê¸°\")\n",
    "\n",
    "    # ë™ì‘ ì—°ê²°\n",
    "    run_btn.click(\n",
    "        fn=chat,\n",
    "        inputs=[input_box, situation_radio, chat_radio, state],\n",
    "        outputs=[chat_output, image_output, state]\n",
    "    )\n",
    "\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
